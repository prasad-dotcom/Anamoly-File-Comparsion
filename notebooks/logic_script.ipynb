{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************************************************************************************************\n",
    "#                                                                                                                           *\n",
    "# INSTRUCTION: FOR VENDOR PARTNER USING THIS TDD REVIEW PROGRAM...                                                          *\n",
    "# PLEASE DOWNLOAD THE LATEST CONFIGURATION FILE FROM THE LINK BELOW AND STORE IN LOCAL FOLDER BEFORE YOU RUN THIS PROGRAM   *\n",
    "# Ctrl+ https://teams.mdlz.com/:x:/s/dadocumentrepository/EXa_rN8xWx5IrabH9JLa8JoBeVKwza8egKYlBqVcJi73rA?e=EfgEI2           *\n",
    "#                                                                                                                           *\n",
    "#****************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries & packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from flask import request\n",
    "import sys, os\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters\n",
    "\n",
    "\n",
    "validchar_list1 = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\"]\n",
    "validchar_list2 = [\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n",
    "validchar_list3 = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"_\"]\n",
    "validchar_list4 = [\".\"] # v7.0\n",
    "validchar_list5 = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"] # v7.0\n",
    "validchar_list6 = [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"] # v7.0\n",
    "validchar_list = validchar_list1 + validchar_list2 + validchar_list3\n",
    "validchar_list_iot = validchar_list1 + validchar_list2 + validchar_list3 + validchar_list4  + validchar_list5 + validchar_list6 # v7.0\n",
    "lcase_charlist = validchar_list1 + validchar_list2\n",
    "ignore_list = [\"n/a\", \"n.a\", \"na\", \"tbd\", \"tbc\"]\n",
    "dtyp_list = [\"curr\", \"quan\", \"dec\", \"fltp\", \"int\", \"int64\", \"integer\", \"decimal\", \"int2\", \"int64\", \"int32\"]\n",
    "iot_obj_type_list = [\"stream\", \"realtime\", \"nearrealtime\", \"archival\"] # v7.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare common functions\n",
    "\n",
    "df_log = pd.DataFrame(columns = ['msg-num', 'msg-type', 'xl-worksheet', 'xl-row', 'msg-text'])\n",
    "\n",
    "def log_message(iTyp, iWksht, iRow, iMsg): # function to log a message\n",
    "    global df_log\n",
    "    if iTyp == 'e':\n",
    "        iTyp, iNum = 'error', 0\n",
    "    elif iTyp == 'w':\n",
    "        iTyp, iNum = 'warning', 1\n",
    "    elif iTyp == 'i':\n",
    "        iTyp, iNum = 'info', 2\n",
    "    df_log = pd.concat([df_log, pd.DataFrame([{'msg-num':iNum, 'msg-type':iTyp, 'xl-worksheet':iWksht, 'xl-row':iRow, 'msg-text':iMsg}])], axis=0, ignore_index=True)\n",
    "\n",
    "def log_sort_index(): # function to sort logged messages before display\n",
    "    global df_log\n",
    "    df_log.sort_values(by = ['msg-num', 'xl-worksheet', 'xl-row', 'msg-text'], inplace=True)\n",
    "    df_log.reset_index(drop=True, inplace=True)\n",
    "    log_disp_list = ['msg-type', 'xl-worksheet', 'xl-row', 'msg-text']\n",
    "    df_log = df_log[log_disp_list]\n",
    "\n",
    "def build_error_string(i_obj_type, i_curr_row): # function to build custom error string per object type\n",
    "    if i_obj_type == \"src\":\n",
    "        e1, e2, e3 = \"source\", i_curr_row['source_file'], 'source_field_name'\n",
    "    elif i_obj_type == \"gcs\":\n",
    "        e1, e2, e3 = \"gcs\", i_curr_row['gcs_folder'] + i_curr_row['gcs_file'], 'gcs_field_name'\n",
    "    elif i_obj_type == \"rt\":\n",
    "        e1, e2, e3 = \"raw table\", i_curr_row['raw_dataset'] + \".\" + i_curr_row['raw_table'], 'raw_table_field_name'\n",
    "    elif i_obj_type == \"rv\":\n",
    "        e1, e2, e3 = \"raw view\", i_curr_row['raw_dataset'] + \".\" + i_curr_row['raw_view'], 'raw_view_field_name'\n",
    "    elif i_obj_type == \"ht\":\n",
    "        e1, e2, e3 = \"harmonized table\", i_curr_row['harmonized_dataset'] + \".\" + i_curr_row['harmonized_table'], 'harmonized_table_field_name'\n",
    "    elif i_obj_type == \"hv\":\n",
    "        e1, e2, e3 = \"harmonized view\", i_curr_row['harmonized_dataset'] + \".\" + i_curr_row['harmonized_view'], 'harmonized_view_field_name'\n",
    "    return e1, e2, e3\n",
    "\n",
    "def key_check(i_obj_type, i_curr_row): # function to check and report for table key availability in a file / table / view\n",
    "    tCount = xCount = eCount = oCount = 0\n",
    "    \n",
    "    tCount = len(set(i_curr_row['key']))\n",
    "    xCount = [x.strip().lower() for x in set(i_curr_row['key'])].count('x')\n",
    "    eCount = [x.strip() for x in set(i_curr_row['key'])].count('')\n",
    "    oCount = tCount - xCount - eCount\n",
    "\n",
    "    e1, e2, e3 = build_error_string(i_obj_type, i_curr_row)\n",
    "\n",
    "    if tCount < 1:\n",
    "        e =  e2 + ' has no keys defined'\n",
    "        log_message('e', e1, '-', str(e))\n",
    "    else:\n",
    "        if xCount < 1:\n",
    "            e = e2 + ' has no keys marked X'\n",
    "            log_message('e', e1, '-', str(e))\n",
    "        if oCount > 0:\n",
    "            e = e2 + ' has key values other than X or blank'\n",
    "            log_message('e', e1, '-', str(e))\n",
    "\n",
    "def dim_fact_check(i_obj_type, i_curr_row): # function to check and report dim / fact availability in a file / table / view\n",
    "    tCount = dCount = fCount = oCount = 0\n",
    "    \n",
    "    tCount = len(i_curr_row['dim_/_fact'])\n",
    "    dCount = [x.lower().strip() for x in i_curr_row['dim_/_fact']].count('dim')\n",
    "    fCount = [x.lower().strip() for x in i_curr_row['dim_/_fact']].count('fact')\n",
    "    oCount = tCount - dCount - fCount\n",
    "\n",
    "    e1, e2, e3 = build_error_string(i_obj_type, i_curr_row)\n",
    "    \n",
    "\n",
    "    if tCount < 1:\n",
    "        e = e1 + ' has neither dim(s) nor fact(s)'\n",
    "        log_message('e', e2, '-', str(e))\n",
    "    else:\n",
    "        if dCount < 1:\n",
    "            e = e2 + ' has no dim(s)'\n",
    "            log_message('e', e1, '-', str(e))\n",
    "        if fCount < 1:\n",
    "            e = e2 + ' has no fact(s)'\n",
    "            log_message('w', e1, '-', str(e))\n",
    "        if oCount > 0:\n",
    "            e = e2 + ' has value(s) other than Dim / Fact'\n",
    "            log_message('e', e1, '-', str(e))\n",
    "\n",
    "def duplicate_field_check(i_obj_type, i_curr_row): # function to check and report duplicate fields in a file / table / view\n",
    "    e1, e2, e3 = build_error_string(i_obj_type, i_curr_row)\n",
    "\n",
    "    if len(i_curr_row[e3]) != len(set(i_curr_row[e3])):\n",
    "        e = e2 + ' has duplicate field(s)'\n",
    "        log_message('e',  e1, '-', str(e))\n",
    "\n",
    "def lcase_field_check(i_obj_type, i_curr_row, i_iot_exception:bool): # function to check and report non-lowercase fields in a file / table / view  # v7.0\n",
    "    e1, e2, e3 = build_error_string(i_obj_type, i_curr_row)\n",
    "\n",
    "    if i_iot_exception == False: # v7.0\n",
    "        if [x.lower() for x in i_curr_row[e3]] != i_curr_row[e3]:\n",
    "            e = e2 + ' has field(s) other than lower case'\n",
    "            log_message('e',  e1, '-', str(e))\n",
    "\n",
    "def special_char_field_check(i_obj_type, i_curr_row, i_iot_exception:bool): # function to check and report special char fields in a file / table / view # v7.0\n",
    "    global validchar_list\n",
    "    global validchar_list_iot # v7.0\n",
    "\n",
    "    e1, e2, e3 = build_error_string(i_obj_type, i_curr_row)\n",
    "\n",
    "    for x in i_curr_row[e3]:\n",
    "        invalid_flag = 0\n",
    "        for y in x:\n",
    "            if (y not in validchar_list and i_iot_exception == False) or (y not in validchar_list_iot and i_iot_exception == True): # v7.0\n",
    "               invalid_flag = 1\n",
    "        if invalid_flag == 1:\n",
    "            e = 'field <' + x + '>' + 'in ' + e2 + ' has characters apart from a-z, 0-9, _'\n",
    "            log_message('e', e1, '-', str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg-type</th>\n",
       "      <th>xl-worksheet</th>\n",
       "      <th>xl-row</th>\n",
       "      <th>msg-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>TDD-GLOBAL-PORTFOLIO_REPORTING_PHASE_1-HANA-ACCOLADE_2.13.XLSX review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  msg-type xl-worksheet xl-row  \\\n",
       "0     info            -      -   \n",
       "\n",
       "                                                                msg-text  \n",
       "0  TDD-GLOBAL-PORTFOLIO_REPORTING_PHASE_1-HANA-ACCOLADE_2.13.XLSX review  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#*****************************************************1**********************************************\n",
    "# display function available 1\n",
    "# accept tdd file as input from user\n",
    "\n",
    "df_log.drop(df_log.index, inplace=True) # refresh the message log for this block\n",
    "\n",
    "\n",
    "#file_tdd =\"../static/UPLOAD_FOLDER/TDD-GLOBAL-PORTFOLIO_REPORTING_PHASE_1-HANA-ACCOLADE_2.13.xlsx\"\n",
    "file_tdd = os.getenv('user_file_path')\n",
    "\n",
    "if file_tdd == \"\":\n",
    "    log_message('e', '-', '-', 'tdd not found / supplied')\n",
    "else:\n",
    "    log_message('i', '-', '-', os.path.basename(file_tdd).upper()+' review')\n",
    "\n",
    "if not df_log.empty: # proceed only if error log has data\n",
    "    log_sort_index() # sort the log to display errors, warning, info...in that order\n",
    "    #display(df_log) # display the log\n",
    "    #log_records = df_log.to_dict(orient='records')\n",
    "\n",
    "    output_html = df_log.to_html(classes=\"table table-bordered table-striped\", index=False)\n",
    "    display(HTML(output_html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and store config master\n",
    "\n",
    "\n",
    "df_log.drop(df_log.index, inplace=True) # refresh the message log for this block\n",
    "# set location from where config master has to be read\n",
    "file_cm = \"../static/predefined_file/config_master.xlsx\"\n",
    "\n",
    "try:\n",
    "    df_cm = pd.read_excel(file_cm, sheet_name=\"gcp-config-master\", keep_default_na=False, dtype={'last_update':str}) # read the config master xlsx\n",
    "except Exception as e:\n",
    "    log_message('e','config', '-', str(e)) # log error message if config sheet cannot be read\n",
    "\n",
    "df_cm[\"rowkey\"] = ( # add 'key' to config sheet df to be used later\n",
    "    df_cm[\"region\"] # v6.0\n",
    "    + df_cm[\"data_entity\"]\n",
    "    + df_cm[\"entity_suffix\"]\n",
    "    + df_cm[\"source_system\"]\n",
    "    + df_cm[\"source_object\"]\n",
    "    + df_cm[\"landing_project\"]\n",
    "    + df_cm[\"datalake_project\"]\n",
    "    + df_cm[\"harmonized_dataset\"]\n",
    "    ).str.lower()\n",
    "\n",
    "log_records = [] #created me\n",
    "if not df_log.empty: # proceed only if error log has data\n",
    "    log_sort_index() # sort the log to display errors, warning, info...in that order\n",
    "    #display(df_log) # display the log\n",
    "    log_records = df_log.to_dict(orient='records')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and process tdd xlsx file (summary worksheet) to dataframe\n",
    "\n",
    "df_log.drop(df_log.index, inplace=True) # refresh the message log for this block\n",
    "\n",
    "try:\n",
    "    df_tdd_full = pd.read_excel(file_tdd, sheet_name=None, keep_default_na=False) # read full xlsx as a dictionary in one go first\n",
    "except Exception as e:\n",
    "    log_message('e','tdd', '-', str(e)) # log error message if tdd cannot be read\n",
    "    df_tdd_full = {}\n",
    "try:\n",
    "    df_tdd_summary = df_tdd_full.get('Summary') # get summary sheet\n",
    "    df_tdd_summary.columns = df_tdd_summary.columns.str.replace(' ', '_').str.lower() # replace <space> if any in column names with _\n",
    "except Exception as e:\n",
    "    log_message('e','summary', '-', str(e)) # log error message if summary sheet cannot be read\n",
    "    df_tdd_summary = pd.DataFrame()\n",
    "try: # v8.0\n",
    "    print(df_tdd_summary.head(5))\n",
    "    df_tdd_summary[\"rowkey\"] = ( # add 'key' to summary sheet df to be used later\n",
    "        df_tdd_summary[\"gcp\"] \n",
    "        + df_tdd_summary[\"data_entity\"] \n",
    "        + df_tdd_summary[\"entity_suffix\"] \n",
    "        + df_tdd_summary[\"source_system\"] \n",
    "        + df_tdd_summary[\"source_object\"]\n",
    "        + df_tdd_summary[\"landing_project\"]\n",
    "        + df_tdd_summary[\"data_lake_project\"]\n",
    "        + df_tdd_summary[\"harmonized_dataset\"]\n",
    "        ).str.lower()\n",
    "except: # v8.0\n",
    "    df_tdd_summary[\"rowkey\"] = ( # add 'key' to summary sheet df to be used later # v8.0\n",
    "        df_tdd_summary[\"region\"] # v8.0\n",
    "        + df_tdd_summary[\"data_entity\"] # v8.0\n",
    "        + df_tdd_summary[\"entity_suffix\"] # v8.0\n",
    "        + df_tdd_summary[\"source_system\"] # v8.0\n",
    "        + df_tdd_summary[\"source_object\"] # v8.0\n",
    "        + df_tdd_summary[\"landing_project\"]\n",
    "        + df_tdd_summary[\"data_lake_project\"]\n",
    "        + df_tdd_summary[\"harmonized_dataset\"]\n",
    "        ).str.lower() # v8.0\n",
    "\n",
    "df_tdd_summary.reset_index(drop=True, inplace = True)\n",
    "\n",
    "df_tdd_summary = pd.merge(df_tdd_summary, df_cm[['rowkey','source_object_type']], on ='rowkey', how ='inner') # v7.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_wip = df_cm.loc[df_cm[\"rowkey\"].isin(df_tdd_summary[\"rowkey\"])]\n",
    "df_cm = df_cm_wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and process tdd xlsx file (remaining worksheets)\n",
    "df_log.drop(df_log.index, inplace=True) #created\n",
    "try:\n",
    "    df_tdd_source = df_tdd_full.get('Source') # get source sheet\n",
    "    df_tdd_source.columns = df_tdd_source.columns.str.replace(' ', '_').str.lower() # replace <space> if any in column names with _\n",
    "except Exception as e:\n",
    "    log_message('e','source', '-', str(e)) # log error message if source sheet cannot be read\n",
    "\n",
    "try:\n",
    "    df_tdd_gcs = df_tdd_full.get('GCS') # get gcs sheet\n",
    "    df_tdd_gcs.columns = df_tdd_gcs.columns.str.replace(' ', '_').str.lower() # replace <space> if any in column names with _\n",
    "except Exception as e:\n",
    "    log_message('e','gcs', '-', str(e)) # log error message if gcs sheet cannot be read\n",
    "\n",
    "try:\n",
    "    df_tdd_rt = df_tdd_full.get('Raw Table') # get raw table sheet\n",
    "    df_tdd_rt.columns = df_tdd_rt.columns.str.replace(' ', '_').str.lower() # replace <space> if any in column names with _\n",
    "except Exception as e:\n",
    "    log_message('e','raw table', '-', str(e)) # log error message if raw table sheet cannot be read\n",
    "\n",
    "try:\n",
    "    df_tdd_rv = df_tdd_full.get('Raw View') # get raw view sheet\n",
    "    df_tdd_rv.columns = df_tdd_rv.columns.str.replace(' ', '_').str.lower() # replace <space> if any in column names with _\n",
    "except Exception as e:\n",
    "    log_message('e','raw view', '-', str(e)) # log error message if raw view sheet cannot be read\n",
    "\n",
    "try:\n",
    "    df_tdd_ht = df_tdd_full.get('Harmonized Table') # get harmonized table sheet\n",
    "    df_tdd_ht.columns = df_tdd_ht.columns.str.replace(' ', '_').str.lower() # replace <space> if any in column names with _\n",
    "except Exception as e:\n",
    "    log_message('e','harmonized table', '-', str(e)) # log error message if harmonized table sheet cannot be read\n",
    "\n",
    "try:\n",
    "    df_tdd_hv = df_tdd_full.get('Harmonized View') # get harmonized view sheet\n",
    "    df_tdd_hv.columns = df_tdd_hv.columns.str.replace(' ', '_').str.lower() # replace <space> if any in column names with _\n",
    "except Exception as e:\n",
    "    log_message('e','harmonized view', '-', str(e)) # log error message if harmonized view sheet cannot be read\n",
    "\n",
    "if not df_log.empty: # proceed only if error log has data\n",
    "    log_sort_index() # sort the log to display errors, warning, info...in that order\n",
    "    #display(df_log) # display the log\n",
    "    print(df_log) #created me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********************************************DISPLAY************************************\n",
    "# keep all groupby ready for further use\n",
    "\n",
    "df_log.drop(df_log.index, inplace=True) # refresh the message log for this block\n",
    "\n",
    "try:\n",
    "    try: # v8.0\n",
    "        df_tdd_summ_gp = df_tdd_summary.groupby( # group summary sheet data for later use \n",
    "            ['gcp', 'data_entity', 'entity_suffix', 'source_system', 'source_object'])['source_file'].apply(list).to_frame().reset_index()\n",
    "    except: # v8.0\n",
    "        df_tdd_summ_gp = df_tdd_summary.groupby( # group summary sheet data for later use # v8.0\n",
    "            ['region', 'data_entity', 'entity_suffix', 'source_system', 'source_object'])['source_file'].apply(list).to_frame().reset_index() # v8.0\n",
    "\n",
    "    df_tdd_summ_ht_gp = df_tdd_summary.groupby( # group summary sheet data (customized for harmonized table read) for later use \n",
    "        ['harmonized_dataset', 'harmonized_table']).apply(\n",
    "            lambda x: [list(x['data_entity']), list(x['raw_dataset']), list(x['raw_view']), list(x['source_object_type'])]).apply(pd.Series).reset_index() # v7.0\n",
    "    df_tdd_summ_ht_gp.columns =[\n",
    "        'harmonized_dataset', 'harmonized_table', 'data_entity', 'raw_dataset', 'raw_view', 'source_object_type'] # restate column names # v7.0\n",
    "\n",
    "    df_tdd_summ_hv_gp = df_tdd_summary.groupby( # group summary sheet data (customized for harmonized view read) for later use \n",
    "        ['harmonized_dataset', 'harmonized_view']).apply(\n",
    "            lambda x: [list(x['data_entity']), list(x['raw_dataset']), list(x['raw_view']), list(x['harmonized_table']), list(x['source_object_type'])]).apply(\n",
    "                pd.Series).reset_index() # v7.0\n",
    "    df_tdd_summ_hv_gp.columns =[\n",
    "        'harmonized_dataset', 'harmonized_view', 'data_entity', 'raw_dataset', 'raw_view', 'harmonized_table', 'source_object_type'] # restate column names # v7.0\n",
    "\n",
    "except Exception as e:\n",
    "    log_message('e','summary-1', '-', str(e)) # log error message\n",
    "\n",
    "try:\n",
    "    df_tdd_src_gp = df_tdd_source.groupby( # group source sheet data for later use \n",
    "        ['source_file']).apply(\n",
    "            lambda x: [list(x['source_field_name']), list(x['key']), list(x['dim_/_fact']), list(x['source_field_data_type'])]).apply(\n",
    "                pd.Series).reset_index()\n",
    "    df_tdd_src_gp.columns =[\n",
    "        'source_file', 'source_field_name', 'key', 'dim_/_fact', 'source_field_data_type'] # restate column names\n",
    "except Exception as e:\n",
    "    log_message('e','summary-2', '-', str(e)) # log error message\n",
    "\n",
    "try:\n",
    "    df_tdd_gcs_gp = df_tdd_gcs.groupby( # group gcs sheet data for later use \n",
    "        ['gcs_folder', 'gcs_file']).apply(\n",
    "            lambda x: [list(x['gcs_field_name']), list(x['key']), list(x['dim_/_fact']), list(x['mapping_/_reference_/_calculation'])]).apply(\n",
    "                pd.Series).reset_index()\n",
    "    df_tdd_gcs_gp.columns =[\n",
    "        'gcs_folder', 'gcs_file', 'gcs_field_name', 'key', 'dim_/_fact', 'mapping_/_reference_/_calculation'] # restate column names\n",
    "except Exception as e:\n",
    "    log_message('e','summary-3', '-', str(e)) # log error message\n",
    "\n",
    "try:\n",
    "    df_tdd_rt_gp = df_tdd_rt.groupby( # group raw table sheet data for later use \n",
    "        ['raw_dataset', 'raw_table']).apply(\n",
    "            lambda x: [list(x['raw_table_field_name']), list(x['key']), list(x['dim_/_fact']), list(x['mapping_/_reference_/_calculation'])]).apply(\n",
    "                pd.Series).reset_index()\n",
    "    df_tdd_rt_gp.columns =[\n",
    "        'raw_dataset', 'raw_table', 'raw_table_field_name', 'key', 'dim_/_fact', 'mapping_/_reference_/_calculation'] # restate column names\n",
    "except Exception as e:\n",
    "    log_message('e','summary-4', '-', str(e)) # log error message\n",
    "\n",
    "try:\n",
    "    df_tdd_rv_gp = df_tdd_rv.groupby( # group raw view sheet data for later use \n",
    "        ['raw_dataset', 'raw_view']).apply(\n",
    "            lambda x: [list(x['raw_view_field_name']), list(x['key']), list(x['dim_/_fact']), list(x['mapping_/_reference_/_calculation'])]).apply(\n",
    "                pd.Series).reset_index()\n",
    "    df_tdd_rv_gp.columns =[\n",
    "        'raw_dataset', 'raw_view', 'raw_view_field_name', 'key', 'dim_/_fact', 'mapping_/_reference_/_calculation'] # restate column names\n",
    "except Exception as e:\n",
    "    log_message('e','summary-5', '-', str(e)) # log error message\n",
    "\n",
    "try:\n",
    "    df_tdd_ht_gp = df_tdd_ht.groupby( # group harmonized table sheet data for later use \n",
    "        ['harmonized_dataset', 'harmonized_table']).apply(\n",
    "            lambda x: [list(x['harmonized_table_field_name']), list(x['key']), list(x['dim_/_fact']), list(x['mapping_/_reference_/_calculation'])]).apply(\n",
    "                pd.Series).reset_index()\n",
    "    df_tdd_ht_gp.columns =[\n",
    "        'harmonized_dataset', 'harmonized_table', 'harmonized_table_field_name', 'key', 'dim_/_fact', 'mapping_/_reference_/_calculation'] # restate column names\n",
    "except Exception as e:\n",
    "    log_message('e','summary-6', '-', str(e)) # log error message\n",
    "\n",
    "try:\n",
    "    df_tdd_hv_gp = df_tdd_hv.groupby( # group harmonized view sheet data for later use \n",
    "        ['harmonized_dataset', 'harmonized_view']).apply(\n",
    "            lambda x: [list(x['harmonized_view_field_name']), list(x['key']), list(x['dim_/_fact']), list(x['mapping_/_reference_/_calculation'])]).apply(\n",
    "                pd.Series).reset_index()\n",
    "    df_tdd_hv_gp.columns =[\n",
    "        'harmonized_dataset', 'harmonized_view', 'harmonized_view_field_name', 'key', 'dim_/_fact', 'mapping_/_reference_/_calculation'] # restate column names\n",
    "except Exception as e:\n",
    "    log_message('e','summary-7', '-', str(e)) # log error message\n",
    "\n",
    "if not df_log.empty: # proceed only if error log has data\n",
    "    log_sort_index() # sort the log to display errors, warning, info...in that order\n",
    "    display(df_log) # display the log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************DISPLAY*******************************\n",
    "# review summary sheet\n",
    "\n",
    "df_log.drop(df_log.index, inplace=True) # refresh the message log for this block\n",
    "\n",
    "# check for duplicate entries in summary\n",
    "try: # v8.0\n",
    "    df_tdd_summary_dupl = df_tdd_summary[df_tdd_summary.duplicated( # prepare to find duplicates and store in a df\n",
    "        ['gcp', 'data_entity', 'entity_suffix', 'source_system', 'source_object', 'harmonized_dataset'])]\n",
    "except: # v8.0\n",
    "    df_tdd_summary_dupl = df_tdd_summary[df_tdd_summary.duplicated( # prepare to find duplicates and store in a df # v8.0\n",
    "        ['region', 'data_entity', 'entity_suffix', 'source_system', 'source_object', 'harmonized_dataset'])] # v8.0\n",
    "if df_tdd_summary_dupl.shape[0] > 0: # if entry exists in duplicated df\n",
    "    for idx in df_tdd_summary_dupl.index: # loop thru all the duplicate entries\n",
    "        e = \"entry <\" + df_tdd_summary_dupl[idx] + '> is duplicated' # prepare error message about the duplicate entry\n",
    "        log_message('e','summary', '-', str(e)) # log error message about the duplicate entry\n",
    "\n",
    "rownum = 0 # reset counter\n",
    "for idx in df_tdd_summary.index: # loop thru all summary sheet rows\n",
    "    rownum = idx + 2 # set counter to match excel row num\n",
    "\n",
    "    curr_row = df_tdd_summary.iloc[idx] # get current row of the summary sheet\n",
    "    \n",
    "    try:\n",
    "        cm_row = df_cm[df_cm[\"rowkey\"] == curr_row[\"rowkey\"]].iloc[0] # get corresponding row in config sheet\n",
    "        curr_row[\"source_object_type\"] = cm_row[\"source_object_type\"] # v7.0\n",
    "    except:\n",
    "        e = \"no matching key exists in config sheet\" # prepare message corresponding config entry not found\n",
    "        log_message('e','summary', rownum, str(e)) # log error corresponding config entry not found\n",
    "        continue # skip current summary sheet row and move to the next one\n",
    "\n",
    "    # prepare message to inform tdd summary entry current project and owner of the entity\n",
    "    e = \"entity= \" + curr_row[\"data_entity\"] + \" being modified; current project= \" + cm_row[\"project\"] + \" ;owner= \" + cm_row[\"requestor\"] + \" check impact if any\"\n",
    "    log_message('i','summary', rownum, str(e)) # log information about current project and owner\n",
    "\n",
    "    # check and log error message if tdd summary source file name does not match with config\n",
    "    e = \"source file mismatch with config sheet\"\n",
    "    log_message('e','summary', rownum, str(e)) if(curr_row[\"source_file\"] != cm_row[\"source_file\"]) else None\n",
    "\n",
    "    # check and log error message if tdd gcs folder name does not match with config\n",
    "    e = \"gcs folder mismatch with config sheet\"\n",
    "    log_message('e','summary', rownum, str(e)) if(curr_row[\"gcs_folder\"] != cm_row[\"gcs_folder\"]) else None\n",
    "\n",
    "    # check and log error message if tdd gcs file name does not match with config\n",
    "    e = \"gcs file mismatch with config sheet\"\n",
    "    log_message('e','summary', rownum, str(e)) if(curr_row[\"gcs_file\"] != cm_row[\"gcs_file\"]) else None\n",
    "\n",
    "    # check and log error message if tdd raw dataset name does not match with config\n",
    "    e = \"raw dataset mismatch with config sheet\"\n",
    "    log_message('e','summary', rownum, str(e)) if(curr_row[\"raw_dataset\"] != cm_row[\"raw_dataset\"]) else None\n",
    "    \n",
    "    # check and log error message if tdd raw table name does not match with config\n",
    "    e = \"raw table mismatch with config sheet\"\n",
    "    log_message('e','summary', rownum, str(e)) if(curr_row[\"raw_table\"] != cm_row[\"raw_table\"]) else None\n",
    "\n",
    "    # check and log error message if tdd raw view name does not match with config\n",
    "    e = \"raw view mismatch with config sheet\"\n",
    "    log_message('e','summary', rownum, str(e)) if(curr_row[\"raw_view\"] != cm_row[\"raw_view\"]) else None\n",
    "\n",
    "    # check and log error message if tdd harmonized dataset name does not match with config\n",
    "    e = \"harmonized dataset mismatch with config sheet\"\n",
    "    log_message('e','summary', rownum, str(e)) if(curr_row[\"harmonized_dataset\"] != cm_row[\"harmonized_dataset\"]) else None\n",
    "    \n",
    "    # check and log error message if tdd harmonized table name does not match with config\n",
    "    e = \"harmonized table mismatch with config sheet\"\n",
    "    log_message('e','summary', rownum, str(e)) if(curr_row[\"harmonized_table\"] != cm_row[\"harmonized_table\"]) else None\n",
    "\n",
    "    # check and log error message if tdd harmonized view name does not match with config\n",
    "    e = \"harmonized view mismatch with config sheet\"\n",
    "    log_message('e','summary', rownum, str(e)) if(curr_row[\"harmonized_view\"] != cm_row[\"harmonized_view\"]) else None\n",
    "\n",
    "    # check and log error message if tdd summary source file does not have entry in source sheet and name not in ignore list\n",
    "    try: \n",
    "        df_tdd_src_gp[df_tdd_src_gp[\"source_file\"] == curr_row[\"source_file\"]].iloc[0][\"source_field_name\"] \n",
    "    except: \n",
    "        e = \"source file does not exist in source sheet\"\n",
    "        log_message('e','summary', rownum, str(e)) if curr_row[\"source_file\"].lower() not in ignore_list else None\n",
    "\n",
    "    # check and log error message if tdd summary gcs folder/file does not have entry in gcs sheet and name not in ignore list\n",
    "    try: \n",
    "        df_tdd_gcs_gp[(df_tdd_gcs_gp[\"gcs_folder\"] == curr_row[\"gcs_folder\"]) & (df_tdd_gcs_gp[\"gcs_file\"] == curr_row[\"gcs_file\"])].iloc[0][\"gcs_field_name\"] \n",
    "    except: \n",
    "        e = \"gcs folder-file does not exist in gcs sheet\"\n",
    "        log_message('e','summary', rownum, str(e)) if (curr_row[\"gcs_folder\"].lower() not in ignore_list) & (curr_row[\"gcs_file\"].lower() not in ignore_list) else None\n",
    "    \n",
    "    # check and log error message if tdd summary raw dataset-table does not have entry in raw table sheet and name not in ignore list\n",
    "    try: \n",
    "        df_tdd_rt_gp[(df_tdd_rt_gp[\"raw_dataset\"] == curr_row[\"raw_dataset\"]) & (df_tdd_rt_gp[\"raw_table\"] == curr_row[\"raw_table\"])].iloc[0][\"raw_table_field_name\"] \n",
    "    except: \n",
    "        e = \"raw dataset-table does not exist in raw table sheet\"\n",
    "        log_message('e','summary', rownum, str(e)) if (curr_row[\"raw_dataset\"].lower() not in ignore_list) & (curr_row[\"raw_table\"].lower() not in ignore_list) \\\n",
    "        else None\n",
    "\n",
    "    # check and log error message if tdd summary raw dataset-view does not have entry in raw view sheet and name not in ignore list\n",
    "    try: \n",
    "        df_tdd_rv_gp[(df_tdd_rv_gp[\"raw_dataset\"] == curr_row[\"raw_dataset\"]) & (df_tdd_rv_gp[\"raw_view\"] == curr_row[\"raw_view\"])].iloc[0][\"raw_view_field_name\"] \n",
    "    except: \n",
    "        e = \"raw dataset-view does not exist in raw table sheet\"\n",
    "        log_message('e','summary', rownum, str(e)) \\\n",
    "        if (curr_row[\"raw_dataset\"].lower() not in ignore_list) & (curr_row[\"raw_view\"].lower() not in ignore_list) else None\n",
    "\n",
    "    # check and log error message if tdd summary harmonized dataset-table does not have entry in harmonized table sheet and name not in ignore list\n",
    "    try: \n",
    "        df_tdd_ht_gp[(df_tdd_ht_gp[\"harmonized_dataset\"] == curr_row[\"harmonized_dataset\"]) \\\n",
    "        & (df_tdd_ht_gp[\"harmonized_table\"] == curr_row[\"harmonized_table\"])].iloc[0][\"harmonized_table_field_name\"]\n",
    "    except: \n",
    "        e = \"harmonized dataset-table does not exist in harmonized table sheet\"\n",
    "        log_message('e','summary', rownum, str(e)) \\\n",
    "        if (curr_row[\"harmonized_dataset\"].lower() not in ignore_list) & (curr_row[\"harmonized_table\"].lower() not in ignore_list) else None\n",
    "\n",
    "    # check and log error message if tdd summary harmonized dataset-view does not have entry in harmonized view sheet and name not in ignore list\n",
    "    try: \n",
    "        df_tdd_hv_gp[(df_tdd_hv_gp[\"harmonized_dataset\"] == curr_row[\"harmonized_dataset\"]) & \\\n",
    "        (df_tdd_hv_gp[\"harmonized_view\"] == curr_row[\"harmonized_view\"])].iloc[0][\"harmonized_view_field_name\"]\n",
    "    except: \n",
    "        e = \"harmonized dataset-view does not exist in harmonized view sheet\"\n",
    "        log_message('e','summary', rownum, str(e)) \\\n",
    "        if (curr_row[\"harmonized_dataset\"].lower() not in ignore_list) & (curr_row[\"harmonized_view\"].lower() not in ignore_list) else None\n",
    "\n",
    "if not df_log.empty: # proceed only if error log has data\n",
    "    log_sort_index() # sort the log to display errors, warning, info...in that order\n",
    "    display(df_log) # display the log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review source sheet\n",
    "\n",
    "df_log.drop(df_log.index, inplace=True) # refresh the message log for this block\n",
    "\n",
    "for idx in df_tdd_src_gp.index: # loop thru all grouped entries in source sheet\n",
    "    curr_row = df_tdd_src_gp.iloc[idx] # get current row\n",
    "\n",
    "\n",
    "    # check and log error message if source file entry does not exist in summary sheet\n",
    "    try: df_tdd_summary[df_tdd_summary['source_file'] == curr_row['source_file']].iloc[0]\n",
    "    except Exception as e:\n",
    "        e = curr_row['source_file'] + \" mismatch with summary\"\n",
    "        log_message('e','source file', '-', str(e))\n",
    "\n",
    "    if curr_row['source_file'].lower() in ignore_list: continue # skip current source sheet row and move to the next\n",
    "\n",
    "    key_check(\"src\", curr_row) # ensure each source file has atleast one key (no value other than X)\n",
    "\n",
    "    dim_fact_check(\"src\", curr_row) # ensure each source file has atleast one dim, possibly atleast one fact and no value other than dim or fact\n",
    "\n",
    "    duplicate_field_check(\"src\", curr_row) # ensure no duplicate fields in source file\n",
    "\n",
    "    # check and log warning message if selective source sheet data types are marked as dim instead of facts\n",
    "    for i in range(len(curr_row[\"source_field_data_type\"])):\n",
    "        if (curr_row[\"source_field_data_type\"][i].lower() in (dtyp_list)) & (curr_row[\"dim_/_fact\"][i].lower() != \"fact\"):\n",
    "            e = \"field <\" + curr_row['source_field_name'][i] + \"> in \" + curr_row['source_file'] + \" is marked as dim insead of fact\"\n",
    "            log_message('w','source', '-', str(e))\n",
    "\n",
    "if not df_log.empty: # proceed only if error log has data\n",
    "    log_sort_index() # sort the log to display errors, warning, info...in that order\n",
    "    #dum(df_log) # display the log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************DISPLAY**************************************\n",
    "# review gcs sheet\n",
    "\n",
    "df_log.drop(df_log.index, inplace=True) # refresh the message log for this block\n",
    "\n",
    "for idx in df_tdd_gcs_gp.index: # loop thru all grouped entries in gcs sheet\n",
    "    curr_row = df_tdd_gcs_gp.iloc[idx] # get current row\n",
    "\n",
    "    try: \n",
    "        # check if this gcs file has a match in summary sheet\n",
    "        summ_row = df_tdd_summary[(df_tdd_summary['gcs_folder'] == curr_row['gcs_folder']) & (df_tdd_summary['gcs_file'] == curr_row['gcs_file'])].iloc[0]\n",
    "\n",
    "        if curr_row['gcs_file'].lower() in ignore_list: continue # skip further checks as the name is in ignore list (N/A, TBD etc...) and move to next gcs file\n",
    "\n",
    "        # now peform preceding document (source) match checks\n",
    "        try: \n",
    "            # escape reconciliation check with source for IOT scenario # v7.0\n",
    "            if not summ_row[\"source_object_type\"] in iot_obj_type_list: # v7.0\n",
    "                # check whether we can get the corresponding source file for this gcs file\n",
    "                src_row = df_tdd_src_gp[df_tdd_src_gp['source_file'] == summ_row['source_file']].iloc[0]\n",
    "                \n",
    "                # now that we have the source file, check if # of fields in gcs file match with the source file\n",
    "                if len(df_tdd_gcs_gp.iloc[idx]['gcs_field_name']) != len(src_row['source_field_name']):\n",
    "                    # issue an error as we expect them to always match\n",
    "                    e = curr_row['gcs_folder'] + curr_row['gcs_file'] + \" field count mismatch with source\"\n",
    "                    log_message('e','gcs file', '-', str(e))\n",
    "                \n",
    "                # now that we have the source file, also let's check whether the gcs file fields refer back to the source file\n",
    "                if any(not item.startswith(src_row['source_file']) for item in curr_row['mapping_/_reference_/_calculation']):\n",
    "                    # issue an error as we expect them to always match\n",
    "                    e = curr_row['gcs_folder'] + curr_row['gcs_file'] + ' has fields which do not refer back to source'\n",
    "                    log_message('e','gcs file', '-', str(e))\n",
    "        except Exception as e:\n",
    "            # issue an error as we expect gcs file to always have a corresponding source file\n",
    "            e = curr_row['gcs_folder'] + curr_row['gcs_file'] + \" mismatch with source\"\n",
    "            log_message('e','gcs file', '-', str(e))\n",
    "    except Exception as e:\n",
    "        # issue an error as gcs file should always have a corresponding entry in summary sheet\n",
    "        e = curr_row['gcs_folder'] + curr_row['gcs_file'] + \" mismatch with summary\"\n",
    "        log_message('e','gcs file', '-', str(e))\n",
    "        continue # move to the next gcs file\n",
    "\n",
    "    key_check(\"gcs\", curr_row) # ensure each gcs file has atleast one key (no value other than X)\n",
    "\n",
    "    dim_fact_check(\"gcs\", curr_row) # ensure each gcs file has atleast one dim, possibly atleast one fact and no value other than dim or fact\n",
    "    \n",
    "    duplicate_field_check(\"gcs\", curr_row) # ensure no duplicate fields in gcs file\n",
    "\n",
    "if not df_log.empty: # proceed only if error log has data\n",
    "    log_sort_index() # sort the log to display errors, warning, info...in that order\n",
    "    display(df_log) # display the log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************DISPLAY***************************\n",
    "# review raw table sheet\n",
    "\n",
    "df_log.drop(df_log.index, inplace=True) # refresh the message log for this block\n",
    "\n",
    "for idx in df_tdd_rt_gp.index: # loop thru all grouped entries in gcs sheet\n",
    "    curr_row = df_tdd_rt_gp.iloc[idx] # get current row\n",
    "\n",
    "    try: \n",
    "        # check if this gcs file has a match in summary sheet\n",
    "        summ_row = df_tdd_summary[(df_tdd_summary['raw_dataset'] == curr_row['raw_dataset']) & (df_tdd_summary['raw_table'] == curr_row['raw_table'])].iloc[0]\n",
    "\n",
    "        if curr_row['raw_table'].lower() in ignore_list: continue # skip further checks as the name is in ignore list (N/A, TBD etc...) and move to next raw table\n",
    "\n",
    "        # now peform preceding document (gcs) match checks\n",
    "        try: \n",
    "            # escape reconciliation check with gcs for IOT scenario # v7.0\n",
    "            if not summ_row[\"source_object_type\"] in iot_obj_type_list: # v7.0\n",
    "                # check whether we can get the corresponding gcs file for this raw table\n",
    "                gcs_row = df_tdd_gcs_gp[(df_tdd_gcs_gp['gcs_folder'] == summ_row['gcs_folder']) & (df_tdd_gcs_gp['gcs_file'] == summ_row['gcs_file'])].iloc[0]\n",
    "\n",
    "                # check if gcs file is not N/A\n",
    "                if gcs_row['gcs_file'].lower() not in ignore_list:\n",
    "                    # now that we have the gcs file, check if # of fields in raw table match with the gcs file\n",
    "                    if len(df_tdd_rt_gp.iloc[idx]['raw_table_field_name']) != len(gcs_row['gcs_field_name']):\n",
    "                        # issue an error as we expect them to always match\n",
    "                        e = curr_row['raw_dataset'] + \".\" + curr_row['raw_table'] + \" field count mismatch with gcs file\"\n",
    "                        log_message('e','raw table', '-', str(e))\n",
    "\n",
    "                    # now that we have the gcs file, also let's check whether the raw table fields refer back to the gcs file\n",
    "                    if any(not item.startswith(gcs_row['gcs_folder'] + gcs_row['gcs_file']) for item in curr_row['mapping_/_reference_/_calculation']):\n",
    "                        # issue an error as we expect them to always match\n",
    "                        e = curr_row['raw_dataset'] + \".\" + curr_row['raw_table'] + ' has fields which do not refer back to gcs file'\n",
    "                        log_message('e','raw table', '-', str(e))\n",
    "                else:\n",
    "                    # check whether we can get the corresponding source file for this raw table\n",
    "                    src_row = df_tdd_src_gp[df_tdd_src_gp['source_file'] == summ_row['source_file']].iloc[0]\n",
    "                    \n",
    "                    # now that we have the source file, check if # of fields in raw table match with the source file\n",
    "                    if len(df_tdd_rt_gp.iloc[idx]['raw_table_field_name']) != len(src_row['source_field_name']):\n",
    "                        # issue an error as we expect them to always match\n",
    "                        e = curr_row['raw_dataset'] + \".\" + curr_row['raw_table'] + \" field count mismatch with source file\"\n",
    "                        log_message('w','raw table', '-', str(e))\n",
    "                    \n",
    "                    # now that we have the source file, also let's check whether the raw table fields refer back to the source file\n",
    "                    if any(not item.startswith(src_row['source_file']) for item in curr_row['mapping_/_reference_/_calculation']):\n",
    "                        # issue an error as we expect them to always match\n",
    "                        e = curr_row['raw_dataset'] + \".\" + curr_row['raw_table'] + ' has fields which do not refer back to source file'\n",
    "                        log_message('w','raw table', '-', str(e))\n",
    "        except Exception as e:\n",
    "            # issue an error as we expect raw table to always have a corresponding gcs file\n",
    "            e = curr_row['raw_dataset'] + \".\" + curr_row['raw_table'] + \" mismatch with gcs\"\n",
    "            log_message('e','raw table', '-', str(e))\n",
    "    except Exception as e:\n",
    "        # issue an error as raw table should always have a corresponding entry in summary sheet\n",
    "        e = curr_row['raw_dataset'] + \".\" + curr_row['raw_table'] + \" mismatch with summary\"\n",
    "        log_message('e','raw table', '-', str(e))\n",
    "        continue # move to the next raw table\n",
    "\n",
    "    key_check(\"rt\", curr_row)# ensure each raw table has atleast one key (no value other than X)\n",
    "    \n",
    "    dim_fact_check(\"rt\", curr_row)# ensure each raw table has atleast one dim, possibly atleast one fact and no value other than dim or fact\n",
    "    \n",
    "    duplicate_field_check(\"rt\", curr_row)# ensure no duplicate fields in raw table\n",
    "    \n",
    "    # we need a way to escape lcase and special character rule for iot kind of solutions\n",
    "    if summ_row[\"source_object_type\"] in iot_obj_type_list: # v7.0\n",
    "        iot_obj_type_exception = True # v7.0\n",
    "    else: # v7.0\n",
    "        iot_obj_type_exception = False # v7.0\n",
    "\n",
    "    # ensure field names are in lower case\n",
    "    lcase_field_check(\"rt\", curr_row, iot_obj_type_exception)# ensure field names are in lower case # v7.0\n",
    "\n",
    "    # ensure field names don't contain special characters\n",
    "    special_char_field_check(\"rt\", curr_row, iot_obj_type_exception) # v7.0\n",
    "\n",
    "if not df_log.empty: # proceed only if error log has data\n",
    "    log_sort_index() # sort the log to display errors, warning, info...in that order\n",
    "    display(df_log) # display the log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************DISPLAY*****************************************\n",
    "# review raw view sheet\n",
    "\n",
    "df_log.drop(df_log.index,inplace=True)\n",
    "\n",
    "for idx in df_tdd_rv_gp.index:\n",
    "    curr_row = df_tdd_rv_gp.iloc[idx]\n",
    "\n",
    "    # ensure raw view entries match with summary sheet\n",
    "    try: \n",
    "        summ_row = df_tdd_summary[(df_tdd_summary['raw_dataset'] == curr_row['raw_dataset']) & (df_tdd_summary['raw_view'] == curr_row['raw_view'])].iloc[0]\n",
    "    except Exception as e:\n",
    "        e = curr_row['raw_dataset'] + \".\" + curr_row['raw_view'] + \" mismatch with summary\"\n",
    "        log_message('e','raw view', '-', str(e))\n",
    "        continue # move to the next raw view\n",
    "\n",
    "    if curr_row['raw_view'].lower() in ignore_list: continue # move to the next raw view\n",
    "\n",
    "    # now peform preceding document (raw table) match checks\n",
    "    try: \n",
    "        # check whether we can get the corresponding raw table for this raw view\n",
    "        rt_row = df_tdd_rt_gp[(df_tdd_rt_gp['raw_dataset'] == summ_row['raw_dataset']) & (df_tdd_rt_gp['raw_table'] == summ_row['raw_table'])].iloc[0]\n",
    "        # now that we have the raw table, check if # of fields in raw view match with the raw table\n",
    "        if len(df_tdd_rv_gp.iloc[idx]['raw_view_field_name']) != len(rt_row['raw_table_field_name']):\n",
    "            # issue an error as we expect them to always match\n",
    "            e = curr_row['raw_dataset'] + \".\" + curr_row['raw_view'] + \" field count mismatch with raw table\"\n",
    "            log_message('e','raw view', '-', str(e))\n",
    "\n",
    "        # now that we have the raw table, also let's check whether the view fields refer back to the raw table\n",
    "        if any(not item.startswith(rt_row['raw_dataset'] + \".\" + rt_row['raw_table']) for item in curr_row['mapping_/_reference_/_calculation']):\n",
    "            # issue an error as we expect them to always match\n",
    "            e = curr_row['raw_dataset'] + \".\" + curr_row['raw_view'] + ' has fields which do not refer back to raw table'\n",
    "            log_message('e','raw view', '-', str(e))\n",
    "    except Exception as e:\n",
    "            # issue an error as there should have been a corresponding raw table entry according to the summary sheet\n",
    "            e = curr_row['raw_dataset'] + \".\" + curr_row['raw_view'] + \" mismatch with raw table\"\n",
    "            log_message('e','raw view', '-', str(e))\n",
    "\n",
    "\n",
    "    # ensure each raw view has atleast one key (no value other than X or blank)\n",
    "    key_check(\"rv\", curr_row)\n",
    "\n",
    "    # ensure each raw view has atleast one dim, possibly atleast one fact and no value other than dim or fact\n",
    "    dim_fact_check(\"rv\", curr_row)\n",
    "\n",
    "    # ensure no duplicate fields in raw view\n",
    "    duplicate_field_check(\"rv\", curr_row)\n",
    "\n",
    "    # we need a way to escape lcase and special character rule for iot kind of solutions\n",
    "    if summ_row[\"source_object_type\"] in iot_obj_type_list: # v7.0\n",
    "        iot_obj_type_exception = True # v7.0\n",
    "    else: # v7.0\n",
    "        iot_obj_type_exception = False # v7.0\n",
    "\n",
    "    # ensure field names are in lower case\n",
    "    lcase_field_check(\"rv\", curr_row, iot_obj_type_exception)# ensure field names are in lower case # v7.0\n",
    "\n",
    "    # ensure field names don't contain special characters\n",
    "    special_char_field_check(\"rv\", curr_row, iot_obj_type_exception) # v7.0\n",
    "\n",
    "if not df_log.empty:\n",
    "    log_sort_index()\n",
    "    display(df_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************DISPLAY****************************\n",
    "# review harmonized table sheet\n",
    "\n",
    "df_log.drop(df_log.index,inplace=True)\n",
    "\n",
    "for idx in df_tdd_ht_gp.index:\n",
    "    curr_row = df_tdd_ht_gp.iloc[idx]\n",
    "    if curr_row[\"harmonized_dataset\"].lower() in ignore_list or curr_row[\"harmonized_table\"].lower() in ignore_list: # v9.0\n",
    "        continue # v9.0\n",
    "\n",
    "    # ensure harmonized table entries match with summary sheet\n",
    "    try: \n",
    "        # check whether we can get the corresponding summary row for this harmonized table\n",
    "        summ_row = df_tdd_summ_ht_gp[(df_tdd_summ_ht_gp['harmonized_dataset'] == curr_row['harmonized_dataset']) & (\n",
    "            df_tdd_summ_ht_gp['harmonized_table'] == curr_row['harmonized_table'])].iloc[0]\n",
    "        # now that we have summary row, peform preceding document (raw view) match checks\n",
    "        i = 0\n",
    "        for x in summ_row['data_entity']:\n",
    "            if summ_row['raw_view'][i].lower() not in ignore_list:\n",
    "                try: \n",
    "                    # check whether we can get the corresponding raw view for this harmonized table\n",
    "                    rv_row = df_tdd_rv_gp[(df_tdd_rv_gp['raw_dataset'] == summ_row['raw_dataset'][i]) & (df_tdd_rv_gp['raw_view'] == summ_row['raw_view'][i])].iloc[0]\n",
    "                    # now that we have the raw view, check if # of fields in harmonized table match with the raw view\n",
    "                    if len(df_tdd_ht_gp.iloc[idx]['harmonized_table_field_name']) != len(rv_row['raw_view_field_name']):\n",
    "                        # issue a warning only as we do not expect them to always match\n",
    "                        e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_table'] + \" field count mismatch with raw view\"\n",
    "                        log_message('w','harmonized table', '-', str(e))\n",
    "\n",
    "                    # now that we have the raw view, also let's check whether the harmonized table fields refer back to the raw view\n",
    "                    if any(not (rv_row['raw_dataset'] + \".\" + rv_row['raw_view']) in item for item in curr_row['mapping_/_reference_/_calculation']):\n",
    "                        # issue a warning only as we do not expect them to always match\n",
    "                        e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_table'] + ' has fields which do not refer back to raw view'\n",
    "                        log_message('w','harmonized table', '-', str(e))\n",
    "                except Exception as e:\n",
    "                    # issue an error as there should have been a corresponding raw view entry according to the summary sheet\n",
    "                    e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_table'] + \" mismatch with raw view\"\n",
    "                    log_message('e','harmonized table', '-', str(e))\n",
    "            i += 1\n",
    "    except Exception as e:\n",
    "        e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_table'] + \" mismatch with summary\"\n",
    "        log_message('e','harmonized table', '-', str(e))\n",
    "        continue # move to the next harmonized table\n",
    "\n",
    "    if curr_row['harmonized_table'].lower() in ignore_list: continue # move to the next harmonized table\n",
    "\n",
    "    # ensure each harmonized table has atleast one key (no value other than X or blank)\n",
    "    key_check(\"ht\", curr_row)\n",
    "\n",
    "    # ensure each harmonized table has atleast one dim, possibly atleast one fact and no value other than dim or fact\n",
    "    dim_fact_check(\"ht\", curr_row)\n",
    "\n",
    "    # ensure no duplicate fields in harmonized table\n",
    "    duplicate_field_check(\"ht\", curr_row)\n",
    "\n",
    "    # we need a way to escape lcase and special character rule for iot kind of solutions\n",
    "    iot_obj_type_exception = False\n",
    "    for x in summ_row[\"source_object_type\"]:\n",
    "        if x in iot_obj_type_list:\n",
    "            iot_obj_type_exception = True\n",
    "\n",
    "    # ensure field names are in lower case\n",
    "    lcase_field_check(\"ht\", curr_row, iot_obj_type_exception)# ensure field names are in lower case # v7.0\n",
    "\n",
    "    # ensure field names don't contain special characters\n",
    "    special_char_field_check(\"ht\", curr_row, iot_obj_type_exception) # v7.0\n",
    "\n",
    "if not df_log.empty:\n",
    "    log_sort_index()\n",
    "    display(df_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************DISPLAY*****************************\n",
    "# review harmonized view sheet\n",
    "\n",
    "df_log.drop(df_log.index,inplace=True)\n",
    "\n",
    "for idx in df_tdd_hv_gp.index:\n",
    "    curr_row = df_tdd_hv_gp.iloc[idx]\n",
    "\n",
    "    if curr_row[\"harmonized_dataset\"].lower() in ignore_list or curr_row[\"harmonized_view\"].lower() in ignore_list: # v9.0\n",
    "        continue # v9.0\n",
    "\n",
    "    # ensure harmonized view entries match with summary sheet\n",
    "    try: \n",
    "        # check whether we can get the corresponding summary row for this harmonized view\n",
    "        summ_row = df_tdd_summ_hv_gp[(df_tdd_summ_hv_gp['harmonized_dataset'] == curr_row['harmonized_dataset']) & (\n",
    "            df_tdd_summ_hv_gp['harmonized_view'] == curr_row['harmonized_view'])].iloc[0]\n",
    "        # now that we have summary row, peform preceding document (raw view if harmonized table is not available else harmonized table) match checks\n",
    "        i = 0\n",
    "        for x in summ_row['data_entity']:\n",
    "            if summ_row['harmonized_table'][i].lower() not in ignore_list:\n",
    "                try: \n",
    "                    # check whether we can get the corresponding harmonized table for this harmonized view\n",
    "                    ht_row = df_tdd_ht_gp[(df_tdd_ht_gp['harmonized_dataset'] == curr_row['harmonized_dataset']) & (\n",
    "                        df_tdd_ht_gp['harmonized_table'] == summ_row['harmonized_table'][i])].iloc[0]\n",
    "                    # now that we have the harmonized table, check if # of fields in harmonized view match with the harmonized table\n",
    "                    if len(df_tdd_hv_gp.iloc[idx]['harmonized_view_field_name']) != len(ht_row['harmonized_table_field_name']):\n",
    "                        # issue an error as we do expect them to always match\n",
    "                        e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_view'] + \" field count mismatch with harmonized table\"\n",
    "                        log_message('e','harmonized view', '-', str(e))\n",
    "\n",
    "                    # now that we have the harmonized table, also let's check whether the harmonized view fields refer back to the harmonized table\n",
    "                    if any(not item.startswith(ht_row['harmonized_dataset'] + \".\" + ht_row['harmonized_table']) for item in curr_row['mapping_/_reference_/_calculation']):\n",
    "                        # issue an error as we expect them to always match\n",
    "                        e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_view'] + ' has fields which do not refer back to harmonized table'\n",
    "                        log_message('e','harmonized view', '-', str(e))\n",
    "                except Exception as e:\n",
    "                    # issue an error as there should have been a corresponding harmonized table entry according to the summary sheet\n",
    "                    e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_view'] + \" mismatch with harmonized table\"\n",
    "                    log_message('e','harmonized view', '-', str(e))\n",
    "            else:\n",
    "                if summ_row['raw_view'][i].lower() not in ignore_list:\n",
    "                    try: \n",
    "                        # check whether we can get the corresponding raw view for this harmonized view\n",
    "                        rv_row = df_tdd_rv_gp[(df_tdd_rv_gp['raw_dataset'] == summ_row['raw_dataset'][i]) & (df_tdd_rv_gp['raw_view'] == summ_row['raw_view'][i])].iloc[0]\n",
    "                        # now that we have the raw view, check if # of fields in harmonized view match with the raw view\n",
    "                        if len(df_tdd_hv_gp.iloc[idx]['harmonized_view_field_name']) != len(rv_row['raw_view_field_name']):\n",
    "                            # issue an error as we expect them to always match\n",
    "                            e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_view'] + \" field count mismatch with raw view\"\n",
    "                            log_message('e','harmonized view', '-', str(e))\n",
    "\n",
    "                        # now that we have the raw view, also let's check whether the harmonized view fields refer back to the raw view\n",
    "                        if any(not item.startswith(rv_row['raw_dataset'] + \".\" + rv_row['raw_view']) for item in curr_row['mapping_/_reference_/_calculation']):\n",
    "                            # issue an error as we expect them to always match\n",
    "                            e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_view'] + ' has fields which do not refer back to raw view'\n",
    "                            log_message('e','harmonized view', '-', str(e))\n",
    "                    except Exception as e:\n",
    "                        # issue an error as there should have been a corresponding raw view entry according to the summary sheet\n",
    "                        e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_view'] + \" mismatch with raw view\"\n",
    "                        log_message('e','harmonized table', '-', str(e))\n",
    "            i += 1\n",
    "    except Exception as e:\n",
    "        e = curr_row['harmonized_dataset'] + \".\" + curr_row['harmonized_view'] + \" mismatch with summary\"\n",
    "        log_message('e','harmonized view', '-', str(e))\n",
    "        continue # move to the next harmonized view\n",
    "\n",
    "    if curr_row['harmonized_view'].lower() in ignore_list: continue # move to the next harmonized view\n",
    "\n",
    "    # ensure each harmonized table has atleast one key (no value other than X or blank)\n",
    "    key_check(\"hv\", curr_row)\n",
    "\n",
    "    # ensure each harmonized table has atleast one dim, possibly atleast one fact and no value other than dim or fact\n",
    "    dim_fact_check(\"hv\", curr_row)\n",
    "\n",
    "    # ensure no duplicate fields in harmonized table\n",
    "    duplicate_field_check(\"hv\", curr_row)\n",
    "\n",
    "    # we need a way to escape lcase and special character rule for iot kind of solutions\n",
    "    iot_obj_type_exception = False\n",
    "    for x in summ_row[\"source_object_type\"]:\n",
    "        if x in iot_obj_type_list:\n",
    "            iot_obj_type_exception = True\n",
    "\n",
    "    # ensure field names are in lower case\n",
    "    lcase_field_check(\"hv\", curr_row, iot_obj_type_exception)# ensure field names are in lower case # v7.0\n",
    "\n",
    "    # ensure field names don't contain special characters\n",
    "    special_char_field_check(\"hv\", curr_row, iot_obj_type_exception) # v7.0\n",
    "\n",
    "if not df_log.empty:\n",
    "    log_sort_index()\n",
    "    display(df_log)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
